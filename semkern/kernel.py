import imp
from typing import List

import numpy as np
from gensim.models import Word2Vec

from semkern.model import most_similar
from semkern.utils import distinct


def distance_matrix(tokens: List[str], model: Word2Vec) -> np.ndarray:
    """
    Calculates the delta distance matrix for tokens based on the vectors generated by the Word2Vec model
    """
    word_vectors = model.wv
    matrix = np.zeros((len(tokens), len(tokens)))
    for i, token in enumerate(tokens):
        # calculates distance of one certain token to every other one
        matrix[i, :] = word_vectors.distances(token, tokens)
    # delete edges between the token and itself
    np.fill_diagonal(matrix, 0.0)
    # Zeroes out connections that are over median distance, this way the graph doesn't become a mess when a lot of nodes are added
    cond = np.median(matrix)
    zero = np.zeros_like(matrix)
    matrix = np.where(matrix > cond, zero, matrix)
    return matrix


def build_kernel(seeds: List[str], k: int, m: int, model: Word2Vec) -> List[str]:
    """
    Builds semantic kernel and returns all token labels.
    Works as follows:
    1. Collects the k most similar words from the models vocabulary to the given seeds, these are called the kernel's types
    2. Collects the m most similar words to the types
    3. Returns all of these words in a list of distinct words, types and seeds are highlighted with capital letters
    """
    types = []
    for seed in seeds:
        types.extend(most_similar(seed, k, model))
        types.append(seed)
    types = distinct(types)
    tokens = []
    for source in types:
        tokens.extend(most_similar(source, m, model))
        tokens.append(source)
    tokens = distinct(tokens)
    labels = [token.upper() if token in types else token.lower() for token in tokens]
    return labels


def distance_matrix(tokens: List[str], model: Word2Vec) -> np.ndarray:
    """
    Calculates the delta distance matrix for tokens based on the vectors generated by the Word2Vec model
    """
    tokens = [token.lower() for token in tokens]
    word_vectors = model.wv
    matrix = np.zeros((len(tokens), len(tokens)))
    for i, token in enumerate(tokens):
        # calculates distance of one certain token to every other one
        matrix[i, :] = word_vectors.distances(token, tokens)
    # delete edges between the token and itself
    np.fill_diagonal(matrix, 0.0)
    # Zeroes out connections that are over median distance, this way the graph doesn't become a mess when a lot of nodes are added
    cond = np.median(matrix)
    zero = np.zeros_like(matrix)
    matrix = np.where(matrix > cond, zero, matrix)
    return matrix
